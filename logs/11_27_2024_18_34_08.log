[ 2024-11-27 18:34:12,380 ] 1038 httpx - INFO - HTTP Request: GET https://dagshub.com/api/v1/user "HTTP/1.1 200 OK"
[ 2024-11-27 18:34:12,384 ] 107 dagshub - INFO - Accessing as deepakpw234
[ 2024-11-27 18:34:12,816 ] 1038 httpx - INFO - HTTP Request: GET https://dagshub.com/api/v1/repos/deepakpw234/Network-Security-Project "HTTP/1.1 200 OK"
[ 2024-11-27 18:34:13,171 ] 1038 httpx - INFO - HTTP Request: GET https://dagshub.com/api/v1/user "HTTP/1.1 200 OK"
[ 2024-11-27 18:34:13,173 ] 107 dagshub - INFO - Initialized MLflow to track repo "deepakpw234/Network-Security-Project"
[ 2024-11-27 18:34:13,181 ] 107 dagshub - INFO - Repository deepakpw234/Network-Security-Project initialized!
[ 2024-11-27 18:35:12,043 ] 30 root - INFO - Start data ingestion
[ 2024-11-27 18:35:12,043 ] 23 root - INFO - Data Ingestion constructor initaited
[ 2024-11-27 18:35:12,043 ] 86 root - INFO - Calling data from mongo DB
[ 2024-11-27 18:35:13,087 ] 88 root - INFO - MongoDB data is loaded into to dataframe
[ 2024-11-27 18:35:13,087 ] 89 root - INFO - Raw data saving in featuer store is started
[ 2024-11-27 18:35:13,143 ] 91 root - INFO - Raw data is saved in feature store
[ 2024-11-27 18:35:13,143 ] 92 root - INFO - Train test split started
[ 2024-11-27 18:35:13,199 ] 94 root - INFO - Train test completed and train and test csv's saved in ingested store
[ 2024-11-27 18:35:13,200 ] 98 root - INFO - Train and test file path saved in the artifact
[ 2024-11-27 18:35:13,200 ] 33 root - INFO - data ingestion completed and data artifact: DataIngestionartifact(train_file_path='artifacts\\27_11_2024_18_34_09\\data_ingestion\\ingested\\train.csv', test_file_path='artifacts\\27_11_2024_18_34_09\\data_ingestion\\ingested\\test.csv')
[ 2024-11-27 18:35:13,200 ] 42 root - INFO - Start data validation
[ 2024-11-27 18:35:13,253 ] 40 root - INFO - required columns are 31
[ 2024-11-27 18:35:13,253 ] 41 root - INFO - dataframe has columns 31
[ 2024-11-27 18:35:13,254 ] 40 root - INFO - required columns are 31
[ 2024-11-27 18:35:13,254 ] 41 root - INFO - dataframe has columns 31
[ 2024-11-27 18:35:13,380 ] 45 root - INFO - data validation completed and data artifact: DataValidationartifact(validation_status=True, valid_train_file_path='artifacts\\27_11_2024_18_34_09\\data_validation\\validated\\train.csv', valid_test_file_path='artifacts\\27_11_2024_18_34_09\\data_validation\\validated\\test.csv', invalid_train_file_path=None, invalid_test_file_path=None, drift_report_file_path='artifacts\\27_11_2024_18_34_09\\data_validation\\drift_report\\report.yaml')
[ 2024-11-27 18:35:13,380 ] 53 root - INFO - Start data transformation
[ 2024-11-27 18:35:13,380 ] 26 root - INFO - Data Transfomation class is initiated with init constructor
[ 2024-11-27 18:35:13,380 ] 55 root - INFO - Initiate the data transformation with initiate function
[ 2024-11-27 18:35:13,419 ] 63 root - INFO - Input and target columns created for training dataframe
[ 2024-11-27 18:35:13,419 ] 69 root - INFO - Input and target columns craeted for test dataframe
[ 2024-11-27 18:35:13,427 ] 42 root - INFO - Saving into numpy array
[ 2024-11-27 18:35:13,429 ] 42 root - INFO - Saving into numpy array
[ 2024-11-27 18:35:13,430 ] 53 root - INFO - Preprocessor is saving into pickle object
[ 2024-11-27 18:35:13,432 ] 53 root - INFO - Preprocessor is saving into pickle object
[ 2024-11-27 18:35:13,435 ] 56 root - INFO - data transformation completed and data artifact: DataTransformationArtifact(transformed_object_file_path='artifacts\\27_11_2024_18_34_09\\data_transformation\\transformed_object\\preprocessing.pkl', transformed_train_file_path='artifacts\\27_11_2024_18_34_09\\data_transformation\\transformed\\train.npy', transformed_test_file_path='artifacts\\27_11_2024_18_34_09\\data_transformation\\transformed\\test.npy')
[ 2024-11-27 18:35:13,435 ] 65 root - INFO - Start model trainer
[ 2024-11-27 18:35:13,435 ] 75 root - INFO - Numpy array is loading
[ 2024-11-27 18:35:13,443 ] 75 root - INFO - Numpy array is loading
